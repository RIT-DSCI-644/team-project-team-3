{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of Saturday March 23:\n",
    "\n",
    "To run this program you need the three datasets and the stopwords file in the same directory as this program.\n",
    "\n",
    "The program does the following:\n",
    "Puts the three tweet datasets into dataframes.\n",
    "Performs sentiment analysis on the tweets using the nltk.vader tool.   This is a lexicon based sentiment analysis trained using social media sources, so we assume it is somewhat applicable.  The analysis is added to the dataframes in two forms, the overall score from -1 to 1 showing magnitude of sentiment, as well as an integer score of -1,0,1 (meaning negative positive neutral) showing only direction of sentiment.  Called Vader_Score and Trinary_Score.\n",
    "\n",
    "A shortcoming of this analysis as is is that any new slang terms or created words or hashtags likely won't be interpretted by the classifier so they'll be simply counted as neutral.  Might miss SOME of the data.\n",
    "\n",
    "March 24: Hand classified some data for test of accuracy of classifier.  Got about 70% accuracy over fifty random data points from the congress dataset.  Seems good enough for our purposes.\n",
    "          Auto exports the sentiment dataframes to program directory - for use by other programs in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import csv\n",
    "import math\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/chris/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Downloads the lexicon used for sentiment anlysis.  Can comment out after run once.\n",
    "nltk.downloader.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the datasets and turn into DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump=pd.read_csv('trump_raw.csv')\n",
    "clinton=pd.read_csv('clinton_raw.csv')\n",
    "congress=pd.read_csv('congress_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords for basic text cleaning for wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "file2=open('stopwords.csv', encoding='utf8')   #file of stopwords from another project...may need to make this file bigger.\n",
    "for stopword in file2.read().split():\n",
    "    stopword = stopword.replace('\"','')\n",
    "    stopwords.append(stopword)\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Wordcloud Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Setting up wordcloud creation.   Takes a list of text entries that we create later.\n",
    "###Second argument is size of the cloud you want returned.  default is top 20 words\n",
    "def make_cloud(words, num=20):\n",
    "    wordcount={}\n",
    "    for line in words:\n",
    "        for word in line.split():\n",
    "            if word in stopwords:\n",
    "                pass\n",
    "        \n",
    "            elif word not in wordcount:\n",
    "                wordcount[word] = 1\n",
    "            else:\n",
    "                wordcount[word] += 1\n",
    "    \n",
    "    d = collections.Counter(wordcount)\n",
    "        \n",
    "    for word, count in d.most_common(num):\n",
    "        print(word, \": \", count)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab All of the text from the tweets from a given DataFrame.  For use with wordcloud generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_text(df):\n",
    "    cw =[]\n",
    "    df = df[\"text\"]   #Grab just the fourth column\n",
    "        \n",
    "    for x in df.index:    #Iterate over the valid indicies. Need this since congresslib/con are partials. \n",
    "        temp=str(df[x])\n",
    "        temp.strip()                  ##Cleans up the text of junk characters\n",
    "        temp=temp.replace('.','')  #stripping out common punctuation so words ending with commas and periods don't count as two different words.\n",
    "        temp=temp.replace(',','')\n",
    "        temp=temp.replace('“','')\n",
    "        temp=temp.replace('”','')\n",
    "        temp=temp.replace('&amp','')\n",
    "        temp=temp.replace(';','')\n",
    "        temp=temp.replace('-',' ')\n",
    "        temp=temp.lower()\n",
    "        cw.append(temp)\n",
    "    return cw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thank :  749\n",
      "great :  663\n",
      "@realdonaldtrump :  534\n",
      "hillary :  467\n",
      "#trump2016 :  453\n",
      "trump :  441\n",
      "#makeamericagreatagain :  295\n",
      "new :  288\n",
      "people :  274\n",
      "america :  274\n",
      "clinton :  258\n",
      "crooked :  224\n",
      "cruz :  200\n",
      "big :  185\n",
      "you! :  185\n",
      "join :  165\n",
      "poll :  164\n",
      "one :  160\n",
      "@cnn :  158\n",
      "going :  149\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Example of word cloud.  Not sure if we'll use this.  We may use its helpr functions if we build our own \n",
    "###classifier...\n",
    "\n",
    "\n",
    "make_cloud(grab_text(trump))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis Stuff follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###The analyzer.  returns a number between [-1,1] with -1 being very negative and 1 being very positive.\n",
    "###Use the compound output as the overall sentiment. (it's some kind of combination of all three attributes)\n",
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Gives a Trinary Poisitive/Negative/Neutral answer.  Will be useful for strict counts of positive/negative/neutral.\n",
    "\n",
    "def vader_polarity(text):\n",
    "    \"\"\" Transform the output to a binary 0/1 result \"\"\"\n",
    "    score = vader.polarity_scores(text)\n",
    "    if score['pos'] > score['neg']:\n",
    "        x=1\n",
    "    elif score['pos'] < score['neg']:\n",
    "        x=-1\n",
    "    else:\n",
    "        x=0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would be nice if @jmartNYT learned how to read the polls before writing his next story. Probably done on purpose, but not good reporting!\n",
      "{'neg': 0.125, 'neu': 0.806, 'pos': 0.07, 'compound': -0.3614}\n",
      "-1\n",
      "\n",
      ".@RobertGBeckel Please thank your brother for his nice words on television. Seems like a great guy and character! @CNN\n",
      "{'neg': 0.0, 'neu': 0.473, 'pos': 0.527, 'compound': 0.9259}\n",
      "1\n",
      "\n",
      "\"@essygalloway: @realDonaldTrump @nbcsnl @Sia  I can't wait to watch snl tomorrow.\" A really big show!\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "##Example of the sentiment analyzer and the trinary classifications\n",
    "##Shows a tweet, the vader nltk.vader analysis, and the trinary classification.\n",
    "\n",
    "x1=trump['text'][0]\n",
    "print(x1)\n",
    "print(vader.polarity_scores(x1))\n",
    "print(str(vader_polarity(x1))+\"\\n\")\n",
    "x2=trump['text'][5]\n",
    "print(x2)\n",
    "print(vader.polarity_scores(x2))\n",
    "print(str(vader_polarity(x2))+\"\\n\")\n",
    "x3=trump['text'][4]\n",
    "print(x3)\n",
    "print(vader.polarity_scores(x3))\n",
    "print(vader_polarity(x3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output from vader gives 4 numbers.  we should use the compound number.  It is calculated using some sort of squishing formula behind the scenes... it's exact function is not important to this analysis and the number should work fine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for analyzing sentiment of all tweets and appending to datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_data_sentiment(df):\n",
    "    Vader_Score = []\n",
    "    Trinary_Score = []\n",
    "    df = df[\"text\"]   #Grab just the fourth column\n",
    "        \n",
    "    for x in df.index:    #Iterate over the valid indicies. Need this since congresslib/con are partials. \n",
    "        temp=str(df[x])\n",
    "        vad_score=vader.polarity_scores(temp)['compound']\n",
    "        trin_score=vader_polarity(temp)\n",
    "        \n",
    "        Vader_Score.append(vad_score)\n",
    "        Trinary_Score.append(trin_score)\n",
    "    return Vader_Score, Trinary_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,z=grab_data_sentiment(trump)\n",
    "trump.insert(0,'Vader_Score',y)\n",
    "trump.insert(0,'Trinary_Score',z)\n",
    "\n",
    "y,z=grab_data_sentiment(clinton)\n",
    "clinton.insert(0,'Vader_Score',y)\n",
    "clinton.insert(0,'Trinary_Score',z)\n",
    "\n",
    "##This one takes a while, large dataset...\n",
    "y,z=grab_data_sentiment(congress)\n",
    "congress.insert(0,'Vader_Score',y)\n",
    "congress.insert(0,'Trinary_Score',z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the dataframe with the sentiment scores inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trinary_Score</th>\n",
       "      <th>Vader_Score</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>AffectCount</th>\n",
       "      <th>MoralCount</th>\n",
       "      <th>shared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-0.3614</td>\n",
       "      <td>11/7/2015 0:07</td>\n",
       "      <td>1824</td>\n",
       "      <td>796</td>\n",
       "      <td>Would be nice if @jmartNYT learned how to read...</td>\n",
       "      <td>11/7/2015</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11/7/2015 0:08</td>\n",
       "      <td>2285</td>\n",
       "      <td>4029</td>\n",
       "      <td>\"@nbcsnl: One more day! Donald Trump hosts #SN...</td>\n",
       "      <td>11/7/2015</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>-0.6616</td>\n",
       "      <td>11/7/2015 3:23</td>\n",
       "      <td>2333</td>\n",
       "      <td>986</td>\n",
       "      <td>\"@Bubbachitchat1: THIS IS WHY THE POLLS ARE WR...</td>\n",
       "      <td>11/7/2015</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>-0.7712</td>\n",
       "      <td>11/7/2015 5:20</td>\n",
       "      <td>3012</td>\n",
       "      <td>1215</td>\n",
       "      <td>One of the dumbest political pundits on televi...</td>\n",
       "      <td>11/7/2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11/7/2015 5:23</td>\n",
       "      <td>1892</td>\n",
       "      <td>703</td>\n",
       "      <td>\"@essygalloway: @realDonaldTrump @nbcsnl @Sia ...</td>\n",
       "      <td>11/7/2015</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Trinary_Score  Vader_Score      created_at  favorite_count  retweet_count  \\\n",
       "0             -1      -0.3614  11/7/2015 0:07            1824            796   \n",
       "1              0       0.0000  11/7/2015 0:08            2285           4029   \n",
       "2             -1      -0.6616  11/7/2015 3:23            2333            986   \n",
       "3             -1      -0.7712  11/7/2015 5:20            3012           1215   \n",
       "4              0       0.0000  11/7/2015 5:23            1892            703   \n",
       "\n",
       "                                                text       time  AffectCount  \\\n",
       "0  Would be nice if @jmartNYT learned how to read...  11/7/2015            1   \n",
       "1  \"@nbcsnl: One more day! Donald Trump hosts #SN...  11/7/2015            0   \n",
       "2  \"@Bubbachitchat1: THIS IS WHY THE POLLS ARE WR...  11/7/2015            0   \n",
       "3  One of the dumbest political pundits on televi...  11/7/2015            1   \n",
       "4  \"@essygalloway: @realDonaldTrump @nbcsnl @Sia ...  11/7/2015            0   \n",
       "\n",
       "   MoralCount  shared  \n",
       "0           0       1  \n",
       "1           0       0  \n",
       "2           0       1  \n",
       "3           1       1  \n",
       "4           0       0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create DFs for separated congress for basic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_numeric(congress['dw_score'])  ##negatives were parsing as strings\n",
    "lib_filter=congress['dw_score']<0\n",
    "con_filter=congress['dw_score']>0\n",
    "congress_lib=pd.DataFrame(congress[lib_filter])  ##Creating copies to get rid of indexing issues.\n",
    "congress_con=pd.DataFrame(congress[con_filter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trinary_Score</th>\n",
       "      <th>Vader_Score</th>\n",
       "      <th>text</th>\n",
       "      <th>elite</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>media</th>\n",
       "      <th>gender</th>\n",
       "      <th>dw_extr</th>\n",
       "      <th>dwextr_rs</th>\n",
       "      <th>dw_score</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>followers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>0.4019</td>\n",
       "      <td>Today is the 1 yr anniversary of the #STEMEduc...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.462</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>71</td>\n",
       "      <td>8386.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-1</td>\n",
       "      <td>-0.6597</td>\n",
       "      <td>Find a Breast Cancer screening provider near y...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.462</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>71</td>\n",
       "      <td>8386.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>October is #BreastCancerAwarenessMonth. Find e...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.462</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>71</td>\n",
       "      <td>8386.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>0.3818</td>\n",
       "      <td>My office is now accepting applications for no...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.462</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>71</td>\n",
       "      <td>8386.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>0.3262</td>\n",
       "      <td>Please stay safe this week NC. For more inform...</td>\n",
       "      <td>Adams</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.462</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>71</td>\n",
       "      <td>8386.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Trinary_Score  Vader_Score  \\\n",
       "41              1       0.4019   \n",
       "42             -1      -0.6597   \n",
       "43              0       0.0000   \n",
       "44              1       0.3818   \n",
       "45              1       0.3262   \n",
       "\n",
       "                                                 text  elite  retweet_count  \\\n",
       "41  Today is the 1 yr anniversary of the #STEMEduc...  Adams              3   \n",
       "42  Find a Breast Cancer screening provider near y...  Adams              0   \n",
       "43  October is #BreastCancerAwarenessMonth. Find e...  Adams              0   \n",
       "44  My office is now accepting applications for no...  Adams              0   \n",
       "45  Please stay safe this week NC. For more inform...  Adams              2   \n",
       "\n",
       "    media  gender  dw_extr  dwextr_rs  dw_score  race  age  followers  \n",
       "41   -0.5    -0.5    0.462      0.462    -0.462  -0.5   71     8386.0  \n",
       "42    0.5    -0.5    0.462      0.462    -0.462  -0.5   71     8386.0  \n",
       "43   -0.5    -0.5    0.462      0.462    -0.462  -0.5   71     8386.0  \n",
       "44   -0.5    -0.5    0.462      0.462    -0.462  -0.5   71     8386.0  \n",
       "45   -0.5    -0.5    0.462      0.462    -0.462  -0.5   71     8386.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_lib.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save copies of the files for use in main program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump.to_csv('trump_sentiment.csv', header=True)\n",
    "clinton.to_csv('clinton_sentiment.csv', header=True)\n",
    "congress.to_csv('congress_sentiment.csv', header=True)\n",
    "congress_lib.to_csv('congress_lib_sentiment.csv', header=True)\n",
    "congress_con.to_csv('congress_con_sentiment.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rough validity of Sentiment Checker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5648      There is nothing better than fresh roasted Georgia pecans... well, except maybe for fresh roasted Georgia... http://t.co/Dsn73cKIuZ'                           \n",
       "7502      Winning the @USDOT #SmartCityChallenge will provide better access to jobs, services, education, &amp; recreation! https://t.co/PgoxoQb5wp'                     \n",
       "12287     Very well said @SpeakerRyan  https://t.co/2yIDIUkAY1'                                                                                                          \n",
       "23130     Congratulations to @Lin_Manuel and @HamiltonMusical for earning 16 #TonyAwards nominations!'                                                                   \n",
       "28065     Go Hillary!!!!  #noflynobuy'                                                                                                                                   \n",
       "39390     Thank you for your critical advocacy.  https://t.co/qEdavFFmiS'                                                                                                \n",
       "45158     Really enjoyed the @AGCSanDiego annual summer mixer and car show. Thanks for having me! https://t.co/9IERnp1xMs'                                               \n",
       "47124     Now headed to Cumberland Fire Department Open House.                                                                                                           \n",
       "49104     When gov counts illegals as residents it allows places like So. Ca. to unfairly get more taxpayer dollars. https://t.co/3H5pFES59k'                            \n",
       "51273     'Let's keep the victims of the Paris terrorist attacks in our prayers. #PrayforParis https://t.co/JLYxP0tnJL\"                                                  \n",
       "52051     Happy Birthday to fellow #WA member and colleague on House Agriculture, @RepNewhouse! https://t.co/7isRZ4K3n7'                                                 \n",
       "54040     Great turnout for the 8th Annual \\xe2\\x80\\x9cWalk of Honor for our Veterans\\xe2\\x80\\x9d on #ArmedForcesDay. https://t.co/FZxDSHStc'                            \n",
       "61605     Hmmmm. Something seems familiar about this situation\\xe2\\x80\\xa6  https://t.co/8aUEQDFh53'                                                                     \n",
       "62533     Astronauts Tim Peake and Jeff Williams to speak to Dallas students from space. watch here- https://t.co/Z6XtDymKGS'                                            \n",
       "70637     Almost 2 million film and TV jobs could be impacted by @FCC #SetTopBox proposal. #UnlockThePlan https://t.co/AQF5Y0Kl4o'                                       \n",
       "86193     Thanks also to @Kathyswan147 @rone_don @TilaRowland and Rep Lichtenegger for joining the discussion on moving the Noranda community forward.'                  \n",
       "90006     My statement on Senate Democrats blocking military, veterans funding: https://t.co/pERRJFz0ea https://t.co/sb2MrkRVvp'                                         \n",
       "90480     Remembering #September11, honoring the lives lost, and expressing gratitude to those who serve &amp; protect our nation https://t.co/akc7YmhEfl'               \n",
       "99053     We will never forget the losses we suffered that terrible day, nor will we forget the heroes of 9/11. https://t.co/Inly6LZhMs'                                 \n",
       "116528    Proud to stand with my fellow CBC members, as well as the CHC and CAPAC to #RestoreTheVRA! @ United\\xe2\\x80\\xa6 https://t.co/cdYKvGeNjW'                       \n",
       "121613    'FACT: Unlike European nations, America's tougher process screens refugees 18-24 months before they even set foot on US soil.\"                                 \n",
       "122830    Check out FREE New #Immigrant Orientation hosted by #USCIS Thurs from 5-8. See flyer for details! https://t.co/boJ7aj03FE'                                     \n",
       "126971    Don't miss anything from the campaign trail. Subscribe to our new podcast, The Audible: https://t.co/TowkpCP8xW https://t.co/TG29HU3W6p\"                       \n",
       "130209    With Obama in office, unemployment rate has dropped to 5% and uninsured rate has hit a new low. That\\xe2\\x80\\x99s definitely worth a \\xe2\\x9d\\xa4\\xef\\xb8\\x8f.'\n",
       "134706    So many great museums in #Amherst. Glad to bring #VisitMA02 tour to @CarleMuseum @YiddishBookCtr @DickinsonMuseum http://t.co/ODOoUZYw5Q                       \n",
       "144552    'Saddened to hear of Justice Scalia's passing. While our opinions often diverged, my condolences are with his friends and family.\"                             \n",
       "147580    \\xe2\\x80\\x9cI fought hard to break through the gridlock and dysfunction in Congress to get this done.\\xe2\\x80\\x9d -Murray on #ESSA https://t.co/jC6o9GVhrx'    \n",
       "149140    'Please RT and help this strong young woman from #Schenectady make this Christmas a 'little more merry'. https://t.co/Ggkur4KhaD\"                              \n",
       "149955    @jeba8768 A few hours ago, I spoke on the House floor about gun violence prevention. VIDEO: http://t.co/XumWG3LkvR'                                            \n",
       "161224    According to the Oregon Forests Resources Institute, forest products support 59,000 Oregon jobs.\\xe2\\x80\\xa6 https://t.co/hutJHzsmiP'                          \n",
       "163667    @_vectorist Thank you for your comment.  Here is my heartfelt response:  https://t.co/ELANdCJgZK'                                                              \n",
       "163801    Topics of the interview were national security, terrorism, US-China relations and the need of human rights and religious freedom in China'                     \n",
       "172112    Over 1,000,000 people have watched our Iran ads. Watch now: https://t.co/1fYYPdUCX3                                                                            \n",
       "179788    Dr. Fichtner ALSO confirmed @RepKevinBrady WEP solution would help BOTH new &amp; current beneficiaries but @BarackObama\\xe2\\x80\\x99s proposal would NOT.'     \n",
       "190748    It is stunning that the IRS has chosen to aid and abet identity thieves instead of protecting innocent victims https://t.co/yLS6I2x9wI'                        \n",
       "196858    .@MLive: \"#Michigan city wins \\'Greatest Midwest Town\\xe2\\x80\\x99 vote\\xe2\\x80\\x9d Congratulations to Traverse City! https://t.co/wOcYuXv5gh'                  \n",
       "201323    Standing up against NC's anti-#LGBT discrimination\\xe2\\x80\\xa6 with a little help from our friends. Thank you, Ringo! #HB2 https://t.co/WJieYFOqaE\"            \n",
       "202463    Discussed important issues facing #NorthDakota w/ @ndcounties- thanks for all you do for our state https://t.co/oFx5tRFdcV'                                    \n",
       "205952    #BigPharma spent $880M on lobbying &amp; contributions from 2006-2015. 8X what NRA &amp; gun lobby recorded #OpioidAwareness https://t.co/xBoZb9vh4Y'          \n",
       "207653    One last look at the the #Utes game-winning touchdown last night. #USCvsUTAH #utpol https://t.co/EttDUowWgP'                                                   \n",
       "222857    Random acts of kindness - another reason why I'm proud to call #Salem home! http://t.co/XiahQw9Ejz                                                             \n",
       "224294    Joined Anthony of @Vets4CommonSens &amp; Rick of @VVAmerica for a conversation on improving care for our #veterans. https://t.co/7DsKdG7ORx'                   \n",
       "231683    For National Women's Health Week, here are tips on how women can stay healthy at every age via @womenshealth. #NWHW https://t.co/vPCyRYHC3d\"                   \n",
       "236334    .@CivilRightsAAG by protecting rights of voters to make their choices, whatever they may be, we do our part to build our more perfect union'                   \n",
       "237445    Recently sat down @ThreePenny to hear frm @MagicHat @harpoonbrewery @LostNationBrew &amp; others abt how to grow the thriving #VTBeer industry'                \n",
       "244870    @CatherineAnaya @azlatinomedia you are very kind! Xo'                                                                                                          \n",
       "262420    What an incredible moment!!!  https://t.co/S6rQ660Eu9'                                                                                                         \n",
       "262860    Great to meet @PeaceCorps volunteers who serve their country this week. Happy 55th Anniversary! https://t.co/Elhf2JLipT'                                       \n",
       "269887    '#OnThisDay in 1846, #US Congress voted in favor of President Polk's request to declare war on #Mexico --&gt; https://t.co/SYV2dNQnwT\"                         \n",
       "274139    Proud to receive perfect 100 score from @HRC for work to support #LGBT Americans. https://t.co/9XMTZMTc1Z'                                                     \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(0)  ###Don't change.  Human input based on the random choices from this...  \"chrissentiment\"\n",
    "congressrand=congress\n",
    "randindexlist=[]\n",
    "for x in congressrand.index:\n",
    "    randindexlist.append(x)\n",
    "random.shuffle(randindexlist)\n",
    "testlist=randindexlist[:50]\n",
    "#testlist=randindexlist[:20]\n",
    "###Originally ran on 20 points.  upped to 50 for better confidence in results\n",
    "testlist=sorted(testlist)\n",
    "testlist\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "congressrand=congressrand.iloc[testlist]\n",
    "congressrand['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chrissentiment=[-1,1,1,1,1,1,1,-1,-1,0,1,1,-1,0,0,1,1,0,0,1,1,1,1,1,1,0,1,1,0,1,1,0,0,1,-1,1,1,1,-1,0,1,1,1,1,1,1,1,1,0,1]\n",
    "#                                       ,great turnout         ,dont miss            ,dr fictner       ##for human error indexing purposes...\n",
    "#chrissentiment=[1,1,-1,1,1,0,-1,-1,1,0,1,1,1,1,-1,1,1,1,1,1]  Was 75% with 20 samples.  Decided to go to 50samples for better confidence in results.\n",
    "len(chrissentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "congressrand.insert(1,'Human_Sentiment',chrissentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total=len(chrissentiment)\n",
    "correct=0\n",
    "for x in range(0,len(chrissentiment)):\n",
    "    if congressrand.iloc[x]['Human_Sentiment']==congressrand.iloc[x]['Trinary_Score']:\n",
    "        correct+=1\n",
    "    \n",
    "correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got 70% correct.    Over large sets it should be good enough.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trinary_Score</th>\n",
       "      <th>Human_Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5648</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7502</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12287</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23130</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28065</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39390</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45158</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47124</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49104</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51273</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52051</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54040</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61605</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62533</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70637</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86193</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90006</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90480</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99053</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116528</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121613</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122830</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126971</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130209</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134706</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144552</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147580</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149140</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149955</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161224</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163667</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163801</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172112</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179788</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190748</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196858</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201323</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202463</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205952</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207653</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222857</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224294</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231683</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236334</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237445</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244870</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262420</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262860</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269887</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274139</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Trinary_Score  Human_Sentiment\n",
       "5648   -1             -1              \n",
       "7502    1              1              \n",
       "12287   1              1              \n",
       "23130   1              1              \n",
       "28065   0              1              \n",
       "39390   1              1              \n",
       "45158   1              1              \n",
       "47124  -1             -1              \n",
       "49104   1             -1              \n",
       "51273  -1              0              \n",
       "52051   1              1              \n",
       "54040   1              1              \n",
       "61605   0             -1              \n",
       "62533   0              0              \n",
       "70637   0              0              \n",
       "86193   1              1              \n",
       "90006  -1              1              \n",
       "90480   1              0              \n",
       "99053   1              0              \n",
       "116528  1              1              \n",
       "121613  1              1              \n",
       "122830  1              1              \n",
       "126971  1              1              \n",
       "130209 -1              1              \n",
       "134706  1              1              \n",
       "144552  1              0              \n",
       "147580 -1              1              \n",
       "149140  1              1              \n",
       "149955 -1              0              \n",
       "161224  1              1              \n",
       "163667  1              1              \n",
       "163801  1              0              \n",
       "172112  0              0              \n",
       "179788  1              1              \n",
       "190748 -1             -1              \n",
       "196858  1              1              \n",
       "201323  1              1              \n",
       "202463  1              1              \n",
       "205952 -1             -1              \n",
       "207653  0              0              \n",
       "222857  1              1              \n",
       "224294  1              1              \n",
       "231683  1              1              \n",
       "236334  1              1              \n",
       "237445  0              1              \n",
       "244870  1              1              \n",
       "262420  0              1              \n",
       "262860  1              1              \n",
       "269887 -1              0              \n",
       "274139  1              1              "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###This just shows what my predictions were and the classified scores on the test set I picked.\n",
    "congressrand[['Trinary_Score','Human_Sentiment']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
